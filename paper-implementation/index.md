---
title: Paper Reimplementation
layout: single
categories: paper-implementation
permalink: /paper-implementation/
classes: wide
author_profile: false
entries_layout: grid
sidebar:
    nav: "Machine_Leaning"

feature_row:
  - image_path: /assets/images/GMLM-main-img-t.jpg
    title: "General Machine Learning Methods"
    excerpt: "This page contains **general machine learning methods**. It includes basic but fundamental methods such as Adam optimization, Xavier initialization, and etc."
    url: "/paper-implementation/ml-methology/"
    btn_label: "More"
    btn_class: "btn--inverse"
  - image_path: /assets/images/NLP-main-img-t.jpg
    alt: "Natural Language Processing (NLP)"
    title: "Natural Language Processing (NLP)"
    excerpt: "This page contains various models and methods used in **NLP** field. Starting with the major Attention model, the LSTM model, in this NLP field, I tried to reimplement the papers on BERT and GPT."
    url: "/paper-implementation/nlp/"
    btn_label: "More"
    btn_class: "btn--inverse"
  - image_path: /assets/images/computer-vision-t.jpg
    title: "Computer Vision (CV)"
    excerpt: "This page contains various models and methods used in **CV**. Here, I've implemented CNN, GAN, and other methods. Starting from AlexNet this page contains various NN models that revolutionized CV."
    url: "/paper-implementation/cv/"
    btn_label: "More"
    btn_class: "btn--inverse"

---

I think the best way to learn **artificial intelligence** is to re-implement the groundbreaking architectures and methods. Here, I posted **source codes** and **explanations** of re-implemented papers while studying artificial intelligence. Please choose the content from the fields below.

{% include feature_row %}
